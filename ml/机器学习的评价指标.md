模型在不同业务场景中要用到的评价指标也不尽相同。

### 混淆矩阵

是一个评估分类问题常用的工具。对于k个类别，就是k*k的表格。对于二分类，将样例根据真实类别和预测类别组合可以划分为：

TP：真正例

FP：假正例

TN：真反例

FN：假反例

### 准确率（精度）

$$
Accuracy = \frac{TP + TN} {TP + FP + TN + FN}
$$

模型预测正确的比例。

它是分类问题中最简单直观的指标，但是在实际应用不常使用，**因为当类别不平衡时，尽管准确率高，但是模型不一定有预测能力**。

### 查准率（精确率）

$$
Precision = \frac{TP} {TP + FP}
$$

表示在所有被模型预测为“正例”的样本中，有多少是**真正**的正例。

关注重点在于模型的”正例“预测可靠，误报少。

> 误报（FP）：实际为负，但是预测为正
>
> 漏报（FN）：实际为正，但是预测为负

### 查全率（召回率）

$$
Recall = \frac{TP} {TP + FN}
$$

表示在所有**真实**为“正例”的样本中，有多少被模型**成功找出来**。

关注重点在于模型”正例“找的**全不全**。

**查准率和查全率是存在矛盾的**

>
>
>精确率、召回率的选择：
>
>精确率：预测为正的有多少是正的
>
>召回率：实际为正的，多少被找到
>
>精确率**不能误报**，召回率**不能漏报**
>
>在“黑链识别”、“涉密内容检查”、“提示词攻击检查”中，**漏报影响较大**。
>
>在更涉及于名誉的之类检查中，**误报是影响较大的**。

平衡点（BEP）是”查准率=查全率“时的取值。

### F1-Score

加权调和平均。
$$
F1 = \frac {2 P R} {P + R}
$$

> 可以自定义PR的权重。
> $$
> F_{\beta} = \frac{(1 + {\beta}^2)PR} {{\beta}^2P + R}
> $$

### 多分类情况

在多分类问题中（比如分类猫、狗、鸟），情况变得复杂。不能只看一个指标。通常的做法是：`One-vs-Rest`策略，将多分类问题拆解成多个二分类：

- 分类器 1：猫 vs (狗, 鸟)
- 分类器 2：狗 vs (猫, 鸟)
- 分类器 3：鸟 vs (猫, 狗)

这样会得到三个独立的二分类混淆矩阵。

#### 宏平均

**先分别计算每个类别的指标，然后再对这些指标求平均**。
$$
Macro\_P = \frac {P_{dog} + P_{cat} + P_{bird}} {3} \\
Macro\_R = \frac {R_{dog} + R_{cat} + R_{bird}} {3} \\
Macro\_F1 = \frac {F1_{dog} + F1_{cat} + F1_{bird}} {3}
$$
特点：

- 平等对待所有类别
- 反应模型在”小类别“上的表现

当关心所有类别的性能，尤其是稀有类别时。

#### 微平均

**先全局汇总，再计算指标。**

它将所有类别的 TP, FP, FN 值**先全部加起来**，形成一个全局的混淆矩阵，然后再基于这个全局矩阵计算指标。
$$
TP\_total = TP\_cat + TP\_dog + TP\_bird \\
...
$$
在微平均中，`Micro-P`和`Micro-R`的计算结果总是相等的。

因此：
$$
Accuracy = Micro\_p = Micro\_R
$$
特点：

- 平等对待所有样本
- 被大类别主导

使用场景：更关注模型在整体数据上的表现时。





[机器学习-最全面的评价指标体系 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/359997979)