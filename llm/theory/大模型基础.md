针对encoder、decoder的特点，引入ELMo的预训练思路，开始出现不同的、对transformer进行优化的思路。

encoder-only架构：bert

encoder-decoder架构：t5

decoder-only架构：gpt



LLM（Large Language Model），中文名为大语言模型或大型语言模型，是一种相较传统语言模型参数量更多、在更大规模语料上进行预训练的语言模型。

一般来说，LLM指的是包含数百亿参数的语言模型，他们在数T token预料上通过多卡分布式集群进行预训练，具备远超传统预训练模型的文本理解与生成能力。只要模型展现出涌现能力，即在一系列复杂任务上表现远超传统预训练模型（bert t5）的能力与潜力，都可以称为LLM。

## LLM 的能力

区别LLM与传统模型的显著特征就是LLM具备**涌现能力**（在同样的模型架构与预训练任务中，某些能力在小型模型中不明显，但是在大型模型中特别突出）。同时还可以进行**上下文学习**（允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新）。

通过使用自然语言描述的多任务数据进行微调，也就是**指令微调**，LLM可以在同样使用指令形式化描述的未见过的任务上表现良好。

LLM通过采用思维链（Chain-of-Thought，CoT）推理策略，可以利用包含推理步骤的提示机制来解决这些任务，从而得出最终答案。

## LLM 的特点

1. 支持多语言
2. 长文本处理
3. 扩展多模态
4. **幻觉**



## 训练 LLM

训练一个完整的LLM需要三个阶段：Pretrain、SFT和RLHF。



![alt text](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.assets/2-0.jpg)

### Pretrain

预训练，LLM的预训练和传统预训练模型类似，使用大量无监督文本对随机初始化的模型参数进行训练。主流LLM几乎采用了decoder-only的架构，训练任务是GPT模型的经典预训练任务：因果语言模型（Causal Language Model，CLM）。

> 因果语言模型建模：和最初的语言模型一致，通过给出上文要求模型预测下一个token来进行训练。

预训练需要庞大的算力资源，分布式训练框架是其必不可少的组成部分。



### SFT

预训练赋予了LLM能力，但是需要将这个能力激活出来。

> 预训练的模型任何问题都可以回答，但是却不知道问题含义。无法与其他下游任务或用户指令适配。

**Supervised Fine-Tuning（有监督微调）**

通过选择训练模型的“通用指令遵循能力”（一般为指令微调的方式）。

指令微调：训练的输入是各种类型的用户指令，需要模型拟合的输出则是希望模型在收到该指令后做出的回复。

```text
input: 你是谁？
output: 瞄~
```

SFT的主要目标是让模型从多种类型、多种风格的指令中获得泛化的指令遵循能力，也就是能够理解并回复用户的指令。

> 指令微调本质上仍然是对模型进行CLM训练，只不过要求模型对指令进行理解和回复而不是简单的预测下一个token，所以模型预测的结果不仅是output，而应该是input + output，只不过input部分不参与loss的计算。

模型能否支持多轮对话，与预训练是没有关系的。模型的多轮对话能力完全来自SFT阶段。如果要使模型支持多轮对话，需要在SFT时将训练数据构造成多轮对话格式。

### RLHF

Reinforcement Learning from Human Feedback（人类反馈强化学习）

> 从功能上出发，可以将 LLM 的训练过程分成预训练与对齐（alignment）两个阶段。预训练的核心作用是赋予模型海量的知识，而所谓对齐，其实就是让模型与人类价值观一致，从而输出人类希望其输出的内容。在这个过程中，SFT 是让 LLM 和人类的指令对齐，从而具有指令遵循能力；而 RLHF 则是从更深层次令 LLM 和人类价值观对齐，令其达到安全、有用、无害的核心标准。

RLHF就是引入强化学习的技术，通过实时的人类反馈令LLM可以给出更令人类满意的回复。



[第四章 大语言模型 (datawhalechina.github.io)](https://datawhalechina.github.io/happy-llm/#/./chapter4/第四章 大语言模型?id=_423-rlhf)